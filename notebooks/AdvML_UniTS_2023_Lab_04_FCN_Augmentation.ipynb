{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Effect of **data augmentation** on a *shallow, linear FCN*\n",
    "\n",
    "Advanced Topics in Machine Learning -- Spring 2023, UniTS\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/ganselmif/adv-ml-units/blob/main/notebooks/AdvML_UniTS_2023_Lab_04_FCN_Augmentation.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High-level overview\n",
    "\n",
    "In the *Lab* that follows, we will explore the effect of **data augmentation** on the structure of learned weights in a *neural network* model. As an example, we will consider a *shallow, linear FCN* (fully-connected network with just 1 layer and no non-linearities) that is trained on the MNIST *classification task*.\n",
    "\n",
    "On a high level, you need to:\n",
    "- Define and train a *shallow, linear FCN* on the MNIST dataset, using *training-set augmentation* by means of *random rotations*;\n",
    "- Extract the learned weights from the trained model;\n",
    "- Visualize the learned weights as *images* and comment on their structure.\n",
    "\n",
    "The rest of the notebook will guide you through more detailed steps you need to follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the datasets and defining augmentation strategies\n",
    "\n",
    "Load the (training and test) *MNIST* datasets as we did in the previous lab, with the following meaningful differences:\n",
    "- Apply random rotations to the dataset used for training, using a rotation range of $[0, 180]$ degrees.\n",
    "- Apply normalization to both datasets, using a mean of $0.1307$ and a standard deviation of $0.3081$ (they are notable, pre-computed values for the MNIST **training** dataset);\n",
    "\n",
    "**Hint**: look up the documentation for the `transforms.RandomRotation` and `transforms.Normalize` classes.\n",
    "\n",
    "**Optional**: Visualize the augmented training dataset, to get a sense of the effect of the augmentation strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model definition and training\n",
    "\n",
    "Define the model and train it on the classification task, as we did in the previous *Lab*. Use fixed-learning-rate *Stochastic Gradient Descent* (with no momentum) as the optimizer. Feel free to experiment with the other hyperparameters.\n",
    "\n",
    "**Optional**: Plot the training loss as the training progresses. Show also test loss at the beginning and at the end of training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weights extraction and visualization\n",
    "\n",
    "Extract the tensors corresponding to learned weights from the trained model (they are stored as the `weights` attribute of the linear layer), and visualize them as images.\n",
    "\n",
    "Comment on the structure of the learned weights, in relation to the nature of the augmentation strategy.\n",
    "\n",
    "**Remark**: in order to be able to visualize the weights as images, they need to be appropriately scaled (as tensor) within the $[0,1]$ range.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Optional I: Training with different augmentation strategies\n",
    "\n",
    "Repeat the same process described above, but using different augmentation strategy, namely:\n",
    "- Apply a central (vertical or horizontal, one only or both) flip to images in the training dataset;\n",
    "- Apply random traslations to images in the training dataset, bound by an arbitrary maximum;\n",
    "\n",
    "Comment on the structure of the learned weights, in relation to the nature of the augmentation strategy.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Optional II: Testing for the invariance of the learned representation\n",
    "\n",
    "Test that the internal representation produced by the weights learned under data augmentation is indeed invariant to the same transformation used for data augmentation. You may follow the guidance given in [this notebook](AdvML_UniTS_2023_Lab_04bis_FCN_Invariance.ipynb).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPkibvpTEMRILBn2/x8IuJj",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

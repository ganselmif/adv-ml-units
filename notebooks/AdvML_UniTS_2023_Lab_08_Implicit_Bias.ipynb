{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8: **Implicit bias** of gradient descent: the case of *linear regression*\n",
    "\n",
    "Advanced Topics in Machine Learning -- Spring 2023, UniTS\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/ganselmif/adv-ml-units/blob/main/notebooks/AdvML_UniTS_2023_Lab_08_Implicit_Bias.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview of the *Lab*\n",
    "\n",
    "In this lab, we will study the *implicit bias* induced by *Gradient Descent* optimization in the simple case of *linear regression*, fitted on a *toy* dataset. In particular, we will show that *GD*-optimized weights converge to the **least norm** solution of the *linear regression* problem.\n",
    "\n",
    "An analysis of implicit bias induced by *Stochastic Gradient Descent* in *full-width linear fully-connected* and *full-width linear convolutional* neural networks (which are much more complex and expressive models!) is provided in [this paper](https://arxiv.org/abs/1806.00468).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Linear regression\n",
    "\n",
    "In the case of **linear regression**, fitted by means of *least squares*, we optimize the following loss function:\n",
    "$$\n",
    "L=\\|y-Xw\\|_{2}^{2}\n",
    "$$.\n",
    "\n",
    "If we choose the *GD* optimization algorithm, we perform weight updates proportional to the gradient of the loss function:\n",
    "$$\n",
    "\\nabla_{w} L = -X(y-Xw)\n",
    "$$.\n",
    "\n",
    "Additionally, notice that the **least norm** solution of the *linear regression* problem is given by:\n",
    "$$\n",
    "w^{*}=(X^{T}X)^{-1}X^{T}y\n",
    "$$.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### To-do:\n",
    "\n",
    "The following *toy* dataset is provided:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "m, n = 1000, 10\n",
    "X = np.random.normal(0, 1, (m, n))\n",
    "b = X.dot(np.random.normal(0, 1, n))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Compute the *least norm* solution of the linear regression problem;\n",
    "2. Write a function that computes the gradient of the loss function, as required by *GD* optimization;\n",
    "3. Perform *GD* optimization of the linear regression problem iteratively, storing the weights at each iteration;\n",
    "4. Plot the evolution of the weights during *GD* optimization and comment.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPkibvpTEMRILBn2/x8IuJj",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

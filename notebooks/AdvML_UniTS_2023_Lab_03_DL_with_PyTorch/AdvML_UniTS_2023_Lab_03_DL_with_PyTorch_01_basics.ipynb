{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3.1 | A *whirlwind tour* of `PyTorch`: **the basics**\n",
    "\n",
    "Advanced Topics in Machine Learning -- Fall 2023, UniTS\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/ganselmif/adv-ml-units/blob/main/notebooks//AdvML_UniTS_2023_Lab_03_DL_with_PyTorch/AdvML_UniTS_2023_Lab_03_DL_with_PyTorch_01_basics.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** This notebook is the same as the *solved* version."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[PyTorch](https://pyth.org/) is a Python/C++ framework for:\n",
    "- Efficient numerical computing, with support for strong GPU acceleration & parallelism;\n",
    "- Automatic algorithmic differentiation (mainly in *reverse mode*, *tape-based*; but more recently also in *forward mode*);\n",
    "- Development of deep artificial neural models (a.k.a. *deep learning*);\n",
    "\n",
    "It is also well integrated with the *scientific Python stack*.\n",
    "\n",
    "The flexibility of PyTorch and its *Pythonic* interfaces make it the most widely adopted framework for research and development, both in academia and industry (especially industrial *R&D*).\n",
    "\n",
    "For more info about `PyTorch`, you can have a look at the [official documentation](https://pytorch.org/docs/stable/index.html) or refer to [this book](https://isip.piconepress.com/courses/temple/ece_4822/resources/books/Deep-Learning-with-PyTorch.pdf).  \n",
    "For insights about the inner workings of *autodiff*, you can start exploring the topic from [this survey](https://arxiv.org/abs/1502.05767)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional\n",
    "# pip install icecream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It all begins with...\n",
    "import torch\n",
    "import torch as th  # (Not necessary; a shorthand)\n",
    "\n",
    "import numpy as np  # For comparison\n",
    "\n",
    "import matplotlib.pyplot as plt  # For plotting\n",
    "\n",
    "from icecream import ic  # For pretty-printing variables\n",
    "\n",
    "ic.configureOutput(prefix=\"\\n|> \")  # For pretty-printing variables\n",
    "\n",
    "from torch import Tensor  # For type annotations\n",
    "from numpy.typing import NDArray  # For type annotations\n",
    "from typing import Any  # For type annotations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic operation with `Tensor`s\n",
    "\n",
    "The main building block of PyTorch's linear algebra capabilities is the `Tensor` class. A torch `Tensor` is the (loose) equivalent of NumPy's `ndarray` and most of the functionalities are the same as in NumPy. In general, it is always possible to perform the same logical/mathematical operations typical of NumPy on torch `Tensor`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|> x: tensor([[1, 2, 3],\n",
      "               [> x: tensor([[1, 2, 3],\n",
      "               [4, 5, 6]])\n",
      "|> y: array([[1, 2, 3],\n",
      "              [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "x: Tensor = th.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "y: NDArray[Any] = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "_ = ic(x)\n",
    "_ = ic(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|> x.size(): torch.Size> x.size(): torch.Size([2, 3])\n",
      "|> x.shape: torch.Size([2, 3])\n",
      "|> y.shape: (2, 3)\n"
     ]
    }
   ],
   "source": [
    "# Shapes and sizes\n",
    "_ = ic(x.size())\n",
    "_ = ic(x.shape)\n",
    "\n",
    "_ = ic(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|> x.dtype: torch.int64\n",
      "> x.dtype: torch.int64\n",
      "|> y.dtype: dtype('int64')\n"
     ]
    }
   ],
   "source": [
    "# (d)types\n",
    "_ = ic(x.dtype)\n",
    "_ = ic(y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype of x before casting: torch.int64\n",
      "dtype of x after casting: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# (d)types \"casting\"\n",
    "print(\"dtype of x before casting:\", x.dtype)\n",
    "x: Tensor = x.float()\n",
    "print(\"dtype of x after casting:\", x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtype of x: torch.float16\n"
     ]
    }
   ],
   "source": [
    "# Or with more granular control\n",
    "x: Tensor = x.to(dtype=th.float16)\n",
    "print(\"dtype of x:\", x.dtype)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can build a tensor through the constructor `th.Tensor` (as opposed to `torch.tensor`, mind the capitalisation!). In this case, since `th.Tensor` is an alias for `th.FloatTensor`, the tensor you create will have type `th.float32`.\n",
    "\n",
    "More info on data types [here](https://pyth.org/docs/stable/tensors.html)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor slicing works exactly like in NumPy, by means of square brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|> x: tensor([[[0.2516, 0.5426],> x: tensor([[[0.2516, 0.5426],\n",
      "                [0.3589, 0.3382],\n",
      "                [0.6716, 0.9616]],\n",
      "       \n",
      "               [[0.2841, 0.4554],\n",
      "                [0.1629, 0.5155],\n",
      "                [0.3716, 0.1191]]])\n"
     ]
    }
   ],
   "source": [
    "x: Tensor = th.rand(2, 3, 2)\n",
    "_ = ic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|> x[0, 1, 1]: tensor> x[0, 1, 1]: tensor(0.3382)\n",
      "|> x[0, 1:, 1]: tensor([0.3382, 0.9616])\n",
      "|> x[:, ::2, :]: tensor([[[0.2516, 0.5426],\n",
      "                           [0.6716, 0.9616]],\n",
      "                  \n",
      "                          [[0.2841, 0.4554],\n",
      "                           [0.3716, 0.1191]]])\n"
     ]
    }
   ],
   "source": [
    "_ = ic(x[0, 1, 1])\n",
    "\n",
    "_ = ic(x[0, 1:, 1])\n",
    "\n",
    "_ = ic(x[:, ::2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|> x[0, 1, 1].shape> x[0, 1, 1].shape: torch.Size([])\n",
      "|> th.tensor(3.14).shape: torch.Size([])\n",
      "|> th.tensor([3.14]).shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# Note: 0-dimensional tensors vs 1-dimensional tensors\n",
    "_ = ic(x[0, 1, 1].shape)\n",
    "\n",
    "_ = ic(th.tensor(3.14).shape)\n",
    "\n",
    "_ = ic(th.tensor([3.14]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|> x.numel(): 12\n",
      "> x.numel(): 12\n",
      "|> th.tensor(3.14).numel(): 1\n",
      "|> th.tensor([3.14]).numel(): 1\n"
     ]
    }
   ],
   "source": [
    "# Use of `numel`\n",
    "_ = ic(x.numel())\n",
    "\n",
    "_ = ic(th.tensor(3.14).numel())\n",
    "\n",
    "_ = ic(th.tensor([3.14]).numel())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor reshaping\n",
    "\n",
    "Changing the shape of a tensor can be a crucial operation. To have an idea of its application, just think of `RGB` images.\n",
    "These may be represented as $3\\times H\\times W$ tensors, where H and W stand for height and width of the image (in number of pixels). It is often needed to look at an image as a flattened (1D) vector of pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|> img.shape: torch.Size([> img.shape: torch.Size([3, 8, 8])\n",
      "|> img2.shape: torch.Size([3, 64])\n"
     ]
    }
   ],
   "source": [
    "img: Tensor = th.stack(\n",
    "    tensors=(th.ones(8, 8), th.zeros(8, 8), th.ones(8, 8) / 2), dim=0\n",
    ")\n",
    "\n",
    "img.reshape(\n",
    "    3, 64\n",
    ")  # note that reshaping is not in place, so this call does not change the actual shape of img\n",
    "\n",
    "_ = ic(img.shape)\n",
    "img2: Tensor = img.reshape(3, 64)\n",
    "_ = ic(img2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(img.numpy())    # It errors: `TypeError: Invalid shape (3, 8, 8) for image data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_img: Tensor = img.reshape(8, 8, 3)\n",
    "_ = plt.imshow(new_img.numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "Difference between:\n",
    "- `view`: operations on contiguous memory;\n",
    "- `reshape`: operations on (non-)contiguous memory, using `view` wherever possible;\n",
    "- `permute`: explicit reordering of dimensions and memory;\n",
    "- `.contiguous()`: ensure that the tensor is stored in contiguous memory, with no other modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_img: Tensor = img.permute(1, 2, 0)\n",
    "_ = plt.imshow(new_img.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|> y.size(): torch.Size([3, 2])\n",
      "> y.size(): torch.Size([3, 2])\n",
      "|> y.contiguous().view(6): tensor([1, 4, 2, 5, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "# See also:\n",
    "z: Tensor = th.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "y: Tensor = z.t()  # Transposition does not guarantee memory contiguity!\n",
    "_ = ic(y.size())\n",
    "# ic(y.view(6))   # It errors!\n",
    "_ = ic(y.contiguous().view(6))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|> x @ y: tensor([[> x @ y: tensor([[1.5202, 0.9676, 1.2438, 0.4088],\n",
      "                   [0.9676, 2.0481, 1.2334, 0.7898],\n",
      "                   [1.2438, 1.2334, 1.4995, 0.6067],\n",
      "                   [0.4088, 0.7898, 0.6067, 0.5024]])\n",
      "|> x.matmul(y): tensor([[1.5202, 0.9676, 1.2438, 0.4088],\n",
      "                         [0.9676, 2.0481, 1.2334, 0.7898],\n",
      "                         [1.2438, 1.2334, 1.4995, 0.6067],\n",
      "                         [0.4088, 0.7898, 0.6067, 0.5024]])\n",
      "|> th.matmul(x, y): tensor([[1.5202, 0.9676, 1.2438, 0.4088],\n",
      "                             [0.9676, 2.0481, 1.2334, 0.7898],\n",
      "                             [1.2438, 1.2334, 1.4995, 0.6067],\n",
      "                             [0.4088, 0.7898, 0.6067, 0.5024]])\n"
     ]
    }
   ],
   "source": [
    "x: Tensor = th.rand(4, 5)\n",
    "y: Tensor = x.T  # matrix transposition; also .t()\n",
    "\n",
    "# All the same!\n",
    "_ = ic(x @ y)\n",
    "_ = ic(x.matmul(y))\n",
    "_ = ic(th.matmul(x, y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the operator for matrix multiplication is `@`, not `*`, which indicates the Hadamard (element-wise) product instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|> x * x: tensor([[1.4472e-02, > x * x: tensor([[1.4472e-02, 1.0694e-01, 6.4369e-01, 4.3950e-01, 3.1561e-01],\n",
      "                   [8.5509e-01, 5.4028e-01, 8.2724e-04, 5.8356e-02, 5.9352e-01],\n",
      "                   [2.9618e-01, 4.4405e-02, 6.7896e-01, 8.9673e-03, 4.7096e-01],\n",
      "                   [3.7688e-01, 6.1162e-02, 5.8633e-02, 5.2912e-03, 4.2259e-04]])\n"
     ]
    }
   ],
   "source": [
    "_ = ic(x * x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying a matrix by itself is obviously equivalent to computing its power, and it can be done also by running one of the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|> th.pow(x, 2): tensor([[> th.pow(x, 2): tensor([[1.4472e-02, 1.0694e-01, 6.4369e-01, 4.3950e-01, 3.1561e-01],\n",
      "                          [8.5509e-01, 5.4028e-01, 8.2724e-04, 5.8356e-02, 5.9352e-01],\n",
      "                          [2.9618e-01, 4.4405e-02, 6.7896e-01, 8.9673e-03, 4.7096e-01],\n",
      "                          [3.7688e-01, 6.1162e-02, 5.8633e-02, 5.2912e-03, 4.2259e-04]])\n",
      "|> x**2: tensor([[1.4472e-02, 1.0694e-01, 6.4369e-01, 4.3950e-01, 3.1561e-01],\n",
      "                  [8.5509e-01, 5.4028e-01, 8.2724e-04, 5.8356e-02, 5.9352e-01],\n",
      "                  [2.9618e-01, 4.4405e-02, 6.7896e-01, 8.9673e-03, 4.7096e-01],\n",
      "                  [3.7688e-01, 6.1162e-02, 5.8633e-02, 5.2912e-03, 4.2259e-04]])\n"
     ]
    }
   ],
   "source": [
    "_ = ic(th.pow(x, 2))\n",
    "_ = ic(x**2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in NumPy, there exists a `dot` function to compute the scalar product between vectors. Note that differently from NumPy, in torch this is **not** equivalent to matrix multiplication, as it is intended to work **only with 1D vectors**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|>> v1.shape: torch.Size([4]), v2.shape: torch.Size([4])\n",
      "|> v1.dot(v2): tensor(0.5170)\n",
      "|> v1.matmul(v2): tensor(0.5170)\n",
      "|> v1 @ v2: tensor(0.5170)\n"
     ]
    }
   ],
   "source": [
    "v1: Tensor = x[:, 1]\n",
    "v2: Tensor = x[:, 2]\n",
    "_ = ic(v1.shape, v2.shape)\n",
    "\n",
    "_ = ic(\n",
    "    v1.dot(v2)\n",
    ")  # in the case of 1D vectors, there is no difference between row and column vectors\n",
    "_ = ic(v1.matmul(v2))\n",
    "_ = ic(v1 @ v2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to do something fancier with two vectors, like multiplying a column by a row to obtain a matrix, you need to switch to 2D vectors by reshaping them.\n",
    "\n",
    "When you reshape a tensor, you can leave one dimension unspecified (using -1), as it can be inferred automatically by th."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|> v1.shape: torch.Size([4, > v1.shape: torch.Size([4, 1]), v2.shape: torch.Size([1, 4])\n",
      "|> v1 @ v2: tensor([[0.2624, 0.0094, 0.2695, 0.0792],\n",
      "                     [0.5897, 0.0211, 0.6057, 0.1780],\n",
      "                     [0.1691, 0.0061, 0.1736, 0.0510],\n",
      "                     [0.1984, 0.0071, 0.2038, 0.0599]])\n"
     ]
    }
   ],
   "source": [
    "v1: Tensor = v1.reshape(-1, 1)  # column vector\n",
    "v2: Tensor = v2.reshape(1, -1)  # row vector\n",
    "\n",
    "_ = ic(v1.shape, v2.shape)\n",
    "_ = ic(v1 @ v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ic(v1.dot(v2))    # this doesn't work! dot works only on 1D tensors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduction operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|> x: tensor([[[0.2181, 0.1274],\n",
      "                [0.4803> x: tensor([[[0.2181, 0.1274],\n",
      "                [0.4803, 0.6080],\n",
      "                [0.9447, 0.5521]],\n",
      "       \n",
      "               [[0.1235, 0.7560],\n",
      "                [0.6936, 0.1067],\n",
      "                [0.2273, 0.3385]]])\n"
     ]
    }
   ],
   "source": [
    "x: Tensor = th.rand(2, 3, 2)\n",
    "_ = ic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|> x.sum(): tensor(5.1762)\n",
      "> x.sum(): tensor(5.1762)\n",
      "|> th.sum(x): tensor(5.1762)\n"
     ]
    }
   ],
   "source": [
    "ic(x.sum())\n",
    "_ = ic(th.sum(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|> x.mean(): tensor(0.4313)\n",
      "> x.mean(): tensor(0.4313)\n",
      "|> th.mean(x): tensor(0.4313)\n"
     ]
    }
   ],
   "source": [
    "_ = ic(x.mean())\n",
    "_ = ic(th.mean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|> x.argmin(): tensor(9)\n",
      "> x.argmin(): tensor(9)\n",
      "|> th.argmin(x): tensor(9)\n"
     ]
    }
   ],
   "source": [
    "_ = ic(x.argmin())\n",
    "_ = ic(th.argmin(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is sometimes useful to specify one or more dimensions to reduce (along which you want to perform your operations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|> x.mean(dim=0): tensor([[0.1708, 0.4417],> x.mean(dim=0): tensor([[0.1708, 0.4417],\n",
      "                           [0.5869, 0.3574],\n",
      "                           [0.5860, 0.4453]])\n"
     ]
    }
   ],
   "source": [
    "_ = ic(x.mean(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|> x.argmax(dim=1): tensor([[2, 1],\n",
      "                             [1, 0]])\n",
      "> x.argmax(dim=1): tensor([[2, 1],\n",
      "                             [1, 0]])\n"
     ]
    }
   ],
   "source": [
    "_ = ic(x.argmax(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|> x.sum(dim=(0, 1)): > x.sum(dim=(0, 1)): tensor([2.6875, 2.4887])\n"
     ]
    }
   ],
   "source": [
    "_ = ic(x.sum(dim=(0, 1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Linear regression\n",
    "\n",
    "By using all the pieces we've seen till now, we can build our first *model* using PyTorch: a linear regressor, i.e.:\n",
    "\n",
    "$$\n",
    "y = XW + b\n",
    "$$\n",
    "\n",
    "which can also be simplified as:\n",
    "\n",
    "$$\n",
    "y = XW\n",
    "$$\n",
    "\n",
    "if we incorporate the bias $b$ inside $W$ and add to the $X$ a column of ones to the right.\n",
    "\n",
    "\n",
    "We start by generating our data. We randomly sample $X$ as a $N\\times P$ tensor, meaning that we have 1000 datapoints and 100 features and produce $y$ as:\n",
    "$$\n",
    "y=XM+\\mathcal{N}(0,I)\n",
    "$$\n",
    "where $M$ is a randomly drawn projection vector (shape $P\\times 1$, same as our weights).\n",
    "We are adding some iid gaussian noise on the $y$ to avoid the interpolation regime, in which we could be fitting our data perfectly using a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "N: int = 1000\n",
    "P: int = 100\n",
    "X: Tensor = th.rand(N, P)\n",
    "M: Tensor = th.rand(P, 1)\n",
    "y: Tensor = X @ M + th.normal(mean=th.zeros(N, 1), std=th.ones(N, 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add a column of ones to $X$ to include the bias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X: Tensor = th.cat(tensors=[X, th.ones(N, 1)], dim=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression can be fit with classical statistical methods such as Ordinary Least Squares, and the optimal $W$ has the form:\n",
    "\n",
    "$$\n",
    "W^*=(X^TX)^{-1}X^Ty\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_star: Tensor = ((X.T @ X).inverse()) @ X.T @ y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the quality of this fit we can evaluate the Mean Squared Error (MSE) between the original $y$ and the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|> loss: tensor(0.8603)\n",
      "> loss: tensor(0.8603)\n"
     ]
    }
   ],
   "source": [
    "loss: Tensor = th.nn.functional.mse_loss(input=X @ W_star, target=y)\n",
    "_ = ic(loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why not just `numpy` (1): Automatic differentiation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how ``autograd`` collects gradients. We create two tensors ``a`` and ``b`` with\n",
    "``requires_grad=True``. This signals to ``autograd`` that every operation on them should be tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "a: Tensor = th.tensor([2.0, 3.0], requires_grad=True)\n",
    "b: Tensor = th.tensor([6.0, 4.0], requires_grad=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create another tensor ``Q`` from ``a`` and ``b``.\n",
    "\n",
    "\\begin{align}Q = 3a^3 - b^2\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q: Tensor = 3 * a**3 - b**2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to compute the gradients of ``Q`` w.r.t. ``a`` and ``b``, i.e.:\n",
    "\n",
    "\\begin{align}\\frac{\\partial Q}{\\partial a} = 9a^2\\end{align}\n",
    "\n",
    "\\begin{align}\\frac{\\partial Q}{\\partial b} = -2b\\end{align}\n",
    "\n",
    "We can do this by calling ``.backward()`` on any **scalar** function of ``Q``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True])\n",
      "tensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "# check if collected gradients are correct\n",
    "print(9 * a**2 == a.grad)\n",
    "print(-2 * b == b.grad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why not just `numpy` (2): GPU Acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|> th.matmul(a, b): tensor> th.matmul(a, b): tensor([[256.6814, 249.5139, 255.2641,  ..., 256.0125, 254.6015, 246.8795],\n",
      "                             [261.9914, 254.1315, 253.2242,  ..., 251.7429, 247.9489, 241.0573],\n",
      "                             [252.4773, 243.8860, 251.4451,  ..., 249.3845, 251.5149, 239.8638],\n",
      "                             ...,\n",
      "                             [254.6993, 242.6673, 245.9762,  ..., 248.1128, 247.0720, 238.3586],\n",
      "                             [249.7507, 250.4346, 251.9537,  ..., 250.1937, 241.0707, 238.6906],\n",
      "                             [248.8309, 241.1919, 247.3757,  ..., 249.3362, 242.9924, 234.8426]])\n"
     ]
    }
   ],
   "source": [
    "a: Tensor = th.rand(1000, 1000)\n",
    "b: Tensor = th.rand(1000, 1000)\n",
    "_ = ic(th.matmul(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "    r: Tensor = th.matmul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|> th.matmul(a_cuda, b_cuda): tensor([[256.6814, 249.5140,> th.matmul(a_cuda, b_cuda): tensor([[256.6814, 249.5140, 255.2642,  ..., 256.0125, 254.6014, 246.8796],\n",
      "                                       [261.9914, 254.1314, 253.2242,  ..., 251.7429, 247.9490, 241.0573],\n",
      "                                       [252.4773, 243.8859, 251.4451,  ..., 249.3844, 251.5150, 239.8637],\n",
      "                                       ...,\n",
      "                                       [254.6993, 242.6673, 245.9762,  ..., 248.1129, 247.0721, 238.3586],\n",
      "                                       [249.7505, 250.4346, 251.9537,  ..., 250.1937, 241.0707, 238.6907],\n",
      "                                       [248.8309, 241.1919, 247.3756,  ..., 249.3362, 242.9924, 234.8425]],\n",
      "                                      device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "a_cuda = a.cuda()  # Or, generally: a.to(device)\n",
    "b_cuda = b.cuda()  # Or, generally: a.to(device)\n",
    "_ = ic(th.matmul(a_cuda, b_cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "    r = th.matmul(a_cuda, b_cuda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
